---
title: "model"
author: "Rodolfo Viana"
date: "19-01-2016"
output: html_document
---

To create the model, I used the h2o library, because it is an open-source software for big-data analysis. The h2o is quite fast, flexible, and can carry a large amount of data. It is part of a community that is growing every day.
 
```{r, warning=FALSE, message=FALSE, fig.align='center', echo=FALSE}
# library
library(h2o) 
library(ggplot2)

conn <- h2o.init(nthreads = -1)
```

Initially we split the dataset from the training and validation dataset. This division is important for avoiding the overfitting, which occurs when a super statistical model adapts to the trained set, thus when the model receives a value for which it has not been trained, it will generate a very bad prediction. It is important this division between training and validation to check at what point the model begins to suffer overfitting.
 
```{r, warning=FALSE, message=FALSE, fig.align='center', echo=FALSE}
# Loading files
path_input <- "train.csv"
data <- h2o.importFile(path = path_input, destination_frame = "train.hex", header = TRUE)
 
# Split the data frame. Train 80 %/ Validation  20%
data.split <- h2o.splitFrame(data = data , ratios = 0.80)
 
# Train
data.train <- data.split[[1]]
 
# Validation
data.validacao <- data.split[[2]]
```

My strategy is to first find the best model
We will initially work with GBM models, random forest and GLM. Ideally initially run all models with a large number of trees, great depth and a small learning rate for interaction, but it takes a lot of time on my current machine (with only 4GB)
 
```{r, warning=FALSE, message=FALSE, fig.align='center', echo=FALSE}
# Target
myY <- "target"
 
# Ignored Columns
ignored_columns <- "id"
 
myX <- setdiff(setdiff(names(data.train), myY), ignored_columns)
 
# GBM
gbm <- h2o.gbm(x = myX, build_tree_one_node = T,
            y = myY,
            training_frame    = data.train,
            validation_frame  = data.validacao,
            ntrees            = 100,
            max_depth         = 6,
            learn_rate        = 0.1)
 
# DRF
drf <- h2o.randomForest(x = myX,
                     y = myY,
                     training_frame    = data.train,
                     validation_frame  = data.validacao,
                     ntrees            = 50,
                     max_depth         = 30)
 
# GLM
glm <- h2o.glm(x = myX,
            y = myY,
            training_frame    = data.train,
            validation_frame  = data.validacao,
            lambda            = 1e-5,
            family            = "poisson")
 
# R2 each model 
train_r2_gbm <- h2o.r2(gbm)
test_r2_gbm  <- h2o.r2(gbm, valid = TRUE)
 
train_r2_drf <- h2o.r2(drf)
test_r2_drf  <- h2o.r2(drf, valid = TRUE)
 
train_r2_glm <- h2o.r2(glm)
test_r2_glm  <- h2o.r2(glm, valid = TRUE)
 
df <- data.frame(Rsquared = c(train_r2_gbm, test_r2_gbm, train_r2_drf, test_r2_drf, train_r2_glm, test_r2_glm),
                        type = c("train", "validation", "train", "validation", "train", "validation"),
                        model = c("GBM","GBM","RF", "RF","GLM", "GLM"))
```

```{r, warning=FALSE, message=FALSE, fig.align='center', echo=FALSE}
ggplot(data=df, aes(x = model, y = Rsquared, fill = type)) +
 geom_bar(stat="identity", position=position_dodge()) +
 theme_classic() +
 labs(title = "Models") +
 theme(axis.ticks = element_blank())
```
 
It can see that the WBG had a better result than the other models, thus obtaining a greater Rsquared.
As the GBM was chosen, it is interesting to observe how was the training along the creations of trees. To avoid overfitting divided the training data in training and validation. Thus we can see the exact moment that the model starts to suffer the overfitting.

```{r}
# Loading test
path_test <- "test.csv"
data_test <- h2o.importFile(path = path_test, destination_frame = "test.hex", header = TRUE)
 
target = h2o.predict(object = gbm, newdata = data_test)
h2o.exportFile(target, path = "predict.csv")
```


```{r}
predictions <- read.table("predict.csv", header=TRUE, quote="\"")
test <- read.csv("test.csv", header = TRUE)
output <- data.frame(id = test$id, prediction = predictions$predict)

write.csv(output, file = "final_predict.csv", row.names = FALSE)
```